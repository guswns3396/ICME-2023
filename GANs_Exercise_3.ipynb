{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guswns3396/ICME-2023/blob/main/GANs_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVXPvG8H1Gfh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV-EbhwH1Haw"
      },
      "source": [
        "Link for the dataset used: https://drive.google.com/file/d/1ByPqKC5f9F8ZiJHR5uPMLuCoELdWhwKz/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQO2kdWtQHlA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvi0_jGIQkS4"
      },
      "outputs": [],
      "source": [
        "def get_data(file_name):\n",
        "  data_path = Path('.')\n",
        "  image_path = data_path/'Data'\n",
        "  if image_path.is_dir():\n",
        "    print(\"Data Directory Exists. Skipping Download.\")\n",
        "  else:\n",
        "    print(\"Data Directory being created. Downloading.\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  with zipfile.ZipFile(data_path / file_name, \"r\") as zip_ref:\n",
        "    print(data_path / file_name)\n",
        "    print(\"Unzipping\")\n",
        "    zip_ref.extractall(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcoF2_j_bnIE"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvJitr6BnKkg"
      },
      "outputs": [],
      "source": [
        "gen_dir = 'GenerationHistory'\n",
        "os.makedirs(gen_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "Z722iwupR-m2",
        "outputId": "654ac618-c46a-4df2-eac0-5a43e4287817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Directory being created. Downloading.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2c71d98b0f33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AnimeFacesDatasetKaggle.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-c7030f88b474>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimage_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unzipping\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AnimeFacesDatasetKaggle.zip'"
          ]
        }
      ],
      "source": [
        "get_data(\"AnimeFacesDatasetKaggle.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DcpzdED6pdoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e8f2f4-4795-4f5a-c87c-b8ae855006ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJwKDBaFblya"
      },
      "outputs": [],
      "source": [
        "device = get_device()\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCQKD1kMW0Ed"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 64\n",
        "BATCH_SIZE = 64\n",
        "SCALER = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "LATENT_DIM = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HgsIeAnW6Rq"
      },
      "outputs": [],
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.CenterCrop(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*SCALER)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVPaWk18SDVr"
      },
      "outputs": [],
      "source": [
        "path = \"./Data\"\n",
        "dataset = datasets.ImageFolder(path, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfepY0Q5SHWi"
      },
      "outputs": [],
      "source": [
        "dl = DataLoader(dataset, BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnJ5FmGTWrCb"
      },
      "outputs": [],
      "source": [
        "def unscale(scaled_images):\n",
        "    return scaled_images * SCALER[1][0] + SCALER[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuTFXMGMXkFv"
      },
      "outputs": [],
      "source": [
        "def show_images(images, n_max=64):\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    normal_images = unscale(images.detach()[:n_max])\n",
        "    ax.imshow( make_grid(normal_images, nrow=8).permute(1, 2, 0) )\n",
        "\n",
        "def show_batch(dl, n_max=64):\n",
        "  images, _ = next(iter(dl))\n",
        "  show_images(images, n_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGkx46CHX4lH"
      },
      "outputs": [],
      "source": [
        "show_batch(dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFthQ8-1X7cZ"
      },
      "outputs": [],
      "source": [
        "discriminator = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),   #(3,64,64) -> (64,32,32)\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False), #(64,32,32) -> (128, 16, 16)\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),#(128, 16, 16) -> (256, 8,8)\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),#(256,8,8) -> (512, 4,4)\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),  #(512,4,4) -> (1,1,1)\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTlfBEvWdELM"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(\n",
        "    nn.ConvTranspose2d(LATENT_DIM, 512, kernel_size=4, stride=1, padding=0, bias=False), #(128,1,1) -> (512,4,4)\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),        #(512, 4,4 ) -> (256, 8,8)\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),        #(256,8,8) -> (128, 16,16)\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),         #(128, 16, 16) -> (64, 32,32)\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),           #(64,32,32) -> (3,64,64)\n",
        "    nn.Tanh()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtISHgewddr2"
      },
      "outputs": [],
      "source": [
        "discriminator = discriminator.to(device)\n",
        "generator = generator.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5W1acvEdn26"
      },
      "outputs": [],
      "source": [
        "latent_vector_batch = torch.randn((64,LATENT_DIM,1,1)).to(device)\n",
        "fake_images = generator(latent_vector_batch)\n",
        "print(fake_images.shape)\n",
        "fake_preds = discriminator(fake_images)\n",
        "print(fake_preds.shape)\n",
        "show_images(fake_images.cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlh63BkBgowr"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(real_images, opt_d):\n",
        "  batch_size = real_images.shape[0]\n",
        "  real_targets = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "  latent_vectors = torch.randn(batch_size, LATENT_DIM, 1, 1).to(device)\n",
        "  fake_images = generator(latent_vectors)\n",
        "  fake_targets = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "  opt_d.zero_grad()\n",
        "  fake_preds = discriminator(fake_images)\n",
        "  real_preds = discriminator(real_images)\n",
        "\n",
        "  fake_loss = nn.functional.binary_cross_entropy(fake_preds, fake_targets)\n",
        "  real_loss = nn.functional.binary_cross_entropy(real_preds, real_targets)\n",
        "\n",
        "  loss = real_loss + fake_loss\n",
        "  loss.backward()\n",
        "  opt_d.step()\n",
        "\n",
        "  return loss.detach().cpu().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj6v76MxmJ9E"
      },
      "outputs": [],
      "source": [
        "def train_generator(opt_g):\n",
        "  latent_vectors = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1).to(device)\n",
        "  fake_images = generator(latent_vectors)\n",
        "  fake_targets = torch.ones(BATCH_SIZE, 1).to(device)\n",
        "\n",
        "  opt_g.zero_grad()\n",
        "  outputs = discriminator(fake_images)\n",
        "  loss = nn.functional.binary_cross_entropy(outputs, fake_targets)\n",
        "  loss.backward()\n",
        "  opt_g.step()\n",
        "\n",
        "  return loss.detach().cpu().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-krlkc69mvNk"
      },
      "outputs": [],
      "source": [
        "def save_images(idx, latent_vectors):\n",
        "  fake_images = generator(latent_vectors)\n",
        "  fake_name = F\"FakeImages_{idx}.png\"\n",
        "  save_image(unscale(fake_images), os.path.join(gen_dir, fake_name), nrow=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4khAYuioqaZ"
      },
      "outputs": [],
      "source": [
        "fixed_latent = torch.randn(64, LATENT_DIM, 1, 1, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9H0TY0wouZN"
      },
      "outputs": [],
      "source": [
        "save_images(0, fixed_latent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXsHt8E3o0ya"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs, lr):\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  gen_losses = []\n",
        "  disc_losses = []\n",
        "\n",
        "  opt_d = Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "  opt_g = Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    gen_losses_batch = []\n",
        "    disc_losses_batch = []\n",
        "    for real_images, _ in dl:\n",
        "      real_images = real_images.to(device)\n",
        "      disc_loss = train_discriminator(real_images, opt_d)\n",
        "      gen_loss = train_generator(opt_g)\n",
        "      gen_losses_batch.append(gen_loss)\n",
        "      disc_losses_batch.append(disc_loss)\n",
        "\n",
        "    gen_loss_epoch = torch.tensor(gen_losses_batch).mean()\n",
        "    disc_loss_epoch = torch.tensor(disc_losses_batch).mean()\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} \\t GenLoss: {gen_loss_epoch.item()} \\t DiscLoss: {disc_loss_epoch.item()}\")\n",
        "\n",
        "    gen_losses.append(gen_loss_epoch.item())\n",
        "    disc_losses.append(disc_loss_epoch.item())\n",
        "\n",
        "    save_images(epoch+1, fixed_latent)\n",
        "\n",
        "  return gen_losses, disc_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am-nl7ppq3iw"
      },
      "outputs": [],
      "source": [
        "lr = 0.0002\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHlnnA3Vq7yg"
      },
      "outputs": [],
      "source": [
        "gen_losses, disc_losses = train(num_epochs, lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLawr0jarFH9"
      },
      "outputs": [],
      "source": [
        "real_images, _ = next(iter(dl))\n",
        "real_images = real_images.to(device)\n",
        "batch_size = real_images.shape[0]\n",
        "real_targets = torch.ones(batch_size, 1).to(device)\n",
        "\n",
        "latent_vectors = torch.randn(batch_size, LATENT_DIM, 1, 1).to(device)\n",
        "fake_images = generator(latent_vectors)\n",
        "fake_targets = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "all_images = torch.cat([real_images, fake_images], dim=0)\n",
        "all_targets = torch.cat([real_targets, fake_targets],  dim=0)\n",
        "all_images.shape, all_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Gge_7FT6GEt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpb41M0ex_Zf"
      },
      "source": [
        "##Exercise\n",
        "\n",
        "We have heard so much about how sensitive GANs are while training. Let's see a simpler example of this in real life.\n",
        "\n",
        "In the discriminator training step, I'm passing all my fake examples and fake targets, and gettting the loss over this set...then I'm doing the same for the real images and real targets, and getting the loss.\n",
        "\n",
        "Any self-respecting pytonista would be scandalized by this profligacy. There's a python (and programming in general) rule, DRY...Dont Repeat Yourself.\n",
        "\n",
        "In keeping with this, I should concatenate the real and fake images and the real and fake targets and pass this big batch through the discriminator...thus, simplifying the operation to just one single pass.\n",
        "\n",
        "Please try this. Look at what happens in terms of the generator output and the losses...and explain them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGP_PYjDx9e9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}